# 实验记录

漏了两周的记录
现在开始写

# 2024年3月
## 21日
为了测试工作记忆长度，尝试了翻转和复制任务。<br>
用llama2-7b-chat试了，不进行微调的情况下，只进行few-shot<br>
翻转任务的情况很差，复制任务表现较好<br>
但是复制任务表现好也是有要求的<br>
~~1.不能使用<bos>和<eos>作为表示符，可能是因为他们在训练时有特殊的含义(好像也没有明显差异)~~<br>
~~2.fewshot数量要足够，大概四个，并不是越多样本效果就越好，看上去到达四个之后就变得随机起来了，或许序列本身也有影响<br>~~
3.或许可以只测后一半，前面一半相当于prompt的一部分，这样zero-shot的效果也挺好(其实前面一半也相当于很多个演示了)<br>
4.无意义的token似乎更难记一些, 可以用有意义的试试<br>

复制任务,在窗口内<br>
llama2-7b-chat后一半的正确率在70%~90%左右,无意义的token<br>

试试在PG-19上训练的模型<br>

## 22日
复制任务,在窗口内<br>
对于有意义的token序列，llama2-7b-chat后一半的正确率大部分都是100%， 这么一看，这个设置比较适合测试工作记忆长度<br>

对于在PG-19上从头训练一个epoch的小模型（80M）来说<br>
记忆随机序列的正确率为接近0%, 记忆有意义的序列的在一段区间内为100%<br>
模型在2048的segment上训练，这个区间为4~32，之后就慢慢下降，远远少于训练时的区间<br>
llama2-7b-chat的较高正确率有可能源于语言建模的任务，而这样随机的token序列设置更接近“死记硬背”的工作记忆长度<br>

补充一下，我为什么要执着于不用copy task训练，因为即使在copy task能测出模型工作记忆长度的潜力，但不代表它在正常任务下仍能够有这么长的工作记忆<br>

可以计算语言建模的准确率对比一下，看看记忆（history）的增益<br>
如果前面有没有这个信息准确率都不变，说明根本没在记忆<br>

ok,语言建模的准确率和复制的准确率在训练窗口内有明显变化, 在训练窗口外差距很小。<br>
唯一可惜的是由于参数量较少，并没有达到100%的准确率。<br>

## 25日
配置：Adam，lr=0.001，betas=(0.9, 0.999), eps=1e-08<br>
固定只训练一个epoch（预训练场景），测试了一些超参，batch size影响比较大，或者说更新步数影响比较大。<br>
| data_type | seg_len | seg_num | batch_tokens | params(emb) | valid_ppl | test_ppl | 4x3090_hours |
|-----------|---------|---------|--------------|--------------|------------|----------|--------------|
| shuffle   | 1024    | 32      | 0.5M         | 83M(32M)     | 23.63      | 20.85    | 9.77         |
| shuffle   | 1024    | 32      | 0.25M        | 83M(32M)     | 17.89      | 15.79    | 9.92         |
| shuffle   | 1024    | 32      | 128k         | 83M(32M)     | 17.55      | 15.49    | 10.21        |
| shuffle   | 1024    | 32      | 64k          | 83M(32M)     | 17.4       | 15.35    | 10.77        |

测试了500k,250k,128k,64k四个batch size, 越小效果越好。<br>
时间会越来越长，应该是因为同步梯度的次数变多了<br>
但是考虑到之后的Curriculum Learning场景，还是保留较大的梯度累积步数为好<br>

选择250k tokens(梯度累积步数为8)作为固定的batch size,继续探索学习率的影响<br>


## 27日
xformers 居然比sdpa快1个小时左右，10%


## 28日

WSD调度器并没有在decay阶段出现loss的剧降，看来这个东西没有那么通用。不过确实使ppl降低了很多，但这是相对于没有使用scheduler的情况。
从趋势看，越早使用decay越好。或许使用余弦调度器能得到更好的结果。

| data_type | seg_len | seg_num | batch_tokens | params(emb) | lr       |
|-----------|---------|---------|---------------|-------------|----------|
| shuffle   | 1024    | 32      | 0.25M         | 83M(32M)    | 1.00E-03 |


| type  | warmup_steps | stable_ratio | factor | patience | valid_ppl | test_ppl | 4x3090_hours | 备注          |
|-------|--------------|--------------|--------|----------|-----------|----------|--------------|--------------|
| WSD   | 500          | 0.5          |        |          | 15.6      | 13.78    | 8.46         | 使用xformers  |
| WSD   | 500          | 0.7          |        |          | 15.64     | 13.81    | 9.9          |               |
| WSD   | 500          | 0.9          |        |          | 15.93     | 14.07    | 8.39         | 使用xformers  |

这下不得不补一下余弦调度器的实验了。<br>

还想试一下fp16和bf16混合精度训练的，但是快中期答辩了<br>

最后试一下ReduceLROnPlateau，和CosineAnnealingLR的<br>
吐槽一下,ReduceLROnPlateau几乎步步判定没有improved，几乎退化成MultiStepLR了<br>
而CosineAnnealingLR或许因为T_max太大了(12,000)，看上去挺直的，感觉和LinearLR的差别不是很大。


## 29日

加入KeyNorm, scale改为1.0<br>
开始的时候$||k||\approx3.5$左右，到后面才变成9左右<br>
~~啧，端口冲突也会影响正在跑的程序，下次还是耐心点。<br>~~
照常算的话$\sqrt{d}=8$，$\frac{1}{3.5}=\frac{2.1}{8}$<br>
应该没问题<br>
先把余弦调度器的结果跑出来,确定好超参数再搞模型结构<br>

检查代码commit<br>